---
layout: default
---

# Workshop Description

Deep Learning is a rapidly expanding field with new applications found everyday. In this workshop, we will cover the fundamentals of deep learning for the beginner. We will introduce the math behind training deep learning models: the backpropagation algorithm. Building conceptual understanding of the fundamentals of deep learning will be the focus of the first part of the workshop. We will then cover some of the popular architectures used in deep learning, such as Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), LSTMs, autoencoders and GANs. There will be a hands-on computing tutorial using Jupyter notebooks to build a basic image classification model via transfer learning.  By the end of the workshop, participants will have a firm understanding of the basic terminology and jargon of deep learning and will be prepared to dive into the plethora of online resources and literature available for each specific application area.

_Prerequisites:_ Familiarity of basic concepts from linear algebra, such as vectors and matrices, as well as calculus concepts, such as differentiation. Familiarity with the python programming language and an ability to use Jupyter notebooks will be helpful for the hands-on sessions.

## About the Instructor

![Sherrie Wang](/assets/img/sherrie.jpg){:style="max-width:30%;"}

Sherrie Wang is a PhD student at ICME, advised by Professor David Lobell at the Center on Food Security and the Environment. She works on developing machine learning methods for remote sensing applications, especially in settings where ground truth labels are scarce. These methods are then applied to problems in sustainable agriculture and development, such as mapping where crops are grown in developing countries.

# Workshop Materials

## Pre-workshop Checklist

1. Sign up for [Piazza](https://piazza.com/class/kdpfbm3zrb46kw). We will be using Piazza to answer questions during the workshop.
2. You should have received a welcome email with the Zoom link and password.  Please email us (kunin@stanford.edu) if you haven't.
3. Familiarize yourself with the schedule and see you Thursday August 20th at 9:00 am!

## Schedule

- Session 1 (9:00 AM to 10:30 AM)
  - Introduction
  - Current state of the art in deep learning
  - Math review
  - Architecture of multi-layer neural networks
- Session 2 (10:45 AM to 12:00 PM)
  - Loss functions
  - The backpropgation algorithm
  - The gradient descent algorithm
  - Over-fitting and Under-fitting
- Lunch (12:00 PM to 2:00 PM)
- Session 3 (2:00 PM to 3:15 PM)
  - Convolutional Neural Networks
  - Recurrent Neural Networks
  - Other Architectures
  - Deep Learning Libraries
  - Hands-on coding Session - Tensorflow
- Session 4 (3:30 PM to 4:45 PM)
  - Hands-on coding Session - Keras
  - Hands-on coding Session - Transfer Learning
  - Failures of deep learning

## Slides

* Session 1 - [slides](/assets/img/dlworkshop1.pdf)
* Session 2 - [slides](/assets/img/dlworkshop2.pdf)
* Session 3 - [slides](/assets/img/dlworkshop3.pdf)
* Session 4 - [slides](/assets/img/dlworkshop4.pdf)

## Jupyter Notebooks 

Below links should open the notebooks in [Google Collaboratory](https://colab.research.google.com/), after they open you may have to click "Open in Playground" to be able to run code.

* [TFWalkthrough.ipynb](https://colab.research.google.com/drive/1PhOPsPPYrqLhvuAEPINWWRgQlXTs3QRu)
* [KerasWalkthrough.ipynb](https://colab.research.google.com/drive/1uX27nH7K7UUn0RoQ0mREZ6FSiTv7F4TJ)
* [TransferLearning.ipynb](https://colab.research.google.com/drive/1IRJJd3FUXzjUrz496I0eAMq42k0_JP9n)


## Additional Resources

Here are some additional resources for various topics:

- Calculus Fundamentals
  - [Essence of Calculus](https://www.youtube.com/watch?v=WUvTyaaNkzM&list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr) by Grant Sanderson
- Linear Algebra Fundamentals
  - [Essence of Linear Algebra](https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab) by Grant Sanderson
- Books
  - [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/) by Michael Nielsen - Free online book
  - [Deep Learning](https://www.deeplearningbook.org/) by Ian Goodfellow and Yoshua Bengio and Aaron Courville
- Visualizations
  - [Neural Network Playground](https://playground.tensorflow.org/) - A playground for dense neural networks
  - [Gan Lab](https://poloclub.github.io/ganlab/) - A playground for GANs
  - [Initializing neural networks](https://www.deeplearning.ai/ai-notes/initialization/) - Visual tutorial on initialization in deep learning
  - [Parameter optimization in neural networks](https://www.deeplearning.ai/ai-notes/optimization/) - Visual tutorial on optimization in deep learning
- Stanford Courses
  - [CS 230 Deep Learning](https://cs230.stanford.edu/)
  - [CS 231N Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/)
  - [CS 224N Natural Language Processing with Deep Learning](http://web.stanford.edu/class/cs224n/)
  - [CS 236 Deep Generative Models](https://deepgenerativemodels.github.io/)
- Interesting talks on advanced topics
  - Ben Recht - [Training on test set and other heresies](https://www.youtube.com/watch?v=NTz4rJS9BAI)
  - Aleksander Madry - [A new perspective on Adversarial Perturbations](https://www.youtube.com/watch?v=mUt7w4UoYqM)
